{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset Classification\n",
    "\n",
    "Author: Kyle Hammerberg\n",
    "\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "First of all, what is the random forest algorithm? The random forest algorithm is considered to be an ‘ensemble’ method because it combines multiple decision trees and then the different predictions are aggregated to identify the most popular result among different trees, thus improving accuracy. Ensemble methods are often employed to help reduce variance when datasets contain a lot of noise.  \n",
    "\n",
    "Like all ML algorithms, the RF algorithm comes with its own set of pros and cons. The RF algorithm helps reduce the risk of overfitting (a common problem for simple decision trees) by using a robust number of trees and averaging out the best result. It’s also a very flexible method that allows for easy evaluation for feature importance. Unfortunately, with larger data sets it can be a slow process to make all the necessary computations, but this problem can be mitigated by implementing parallel solutions to optimize model training. RF results are also a bit more complex than a simple decision tree, which comes with the drawback of decreased interpretability.  \n",
    "\n",
    "For more information on implementing parallelization when working with pandas, see this link to learn about using Modin for distributing data and computation to accelerate workflows involving pandas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Data Preprocessing**\n",
    "\n",
    "Alright, let’s get started. \n",
    "\n",
    "We’re going to be working in a Python 3 environment with Jupyter Notebooks.  \n",
    "\n",
    "The first thing you’re going to need to do is make sure you have all of your libraries installed: \n",
    "\n",
    "The libraries that will be required are: \n",
    "\n",
    "NumPy – a library adding support for large, multi-dimensional arrays and matrices, along with high-level mathematical functions to operate on said arrays. \n",
    "\n",
    "Pandas – a library for data analysis, cleaning, and pre-processing. \n",
    "\n",
    "Scikit-learn – a machine learning library that features various classification, regression, and clustering algorithms, including random forests, the algorithm we will be using in this lab.  \n",
    "\n",
    "Matplotlib – a library for creating static, animated, and interactive visualizations in Python. \n",
    "\n",
    "Seaborn – another data visualization library that provides a high-level interface for creating aesthetic and informative statistical graphics. \n",
    "\n",
    "Dtreeviz – a library for decision tree visualization and model interpretation. \n",
    "\n",
    "If you don’t already have a virtual environment set up with the requisite packages, you can use the ‘%pip install’ command in your first notebook kernel.  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requisite libraries  \n",
    "\n",
    "%pip install numpy \n",
    "\n",
    "%pip install pandas \n",
    "\n",
    "%pip install sklearn \n",
    "\n",
    "%pip install matplotlib \n",
    "\n",
    "%pip install seaborn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s import our libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sklearn \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.datasets import load_iris \n",
    "import sklearn.metrics as metrics \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn import tree \n",
    "from dtreeviz.trees import dtreeviz # will be used for tree visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of the necessary packages / librarires are imported, we can start exploring the data. The iris data set comes included with the sklearn library, so it can be assigned to a variable simply by calling the load_iris() function. The data includes 150 4-column arrays containing measurements for each feature, and another 150 1-column arrays with either a 0, 1, or 2 indicating what flower corresponds with each feature array. For more information on the array data structure, follow this link.  \n",
    "\n",
    " \n",
    "\n",
    "For this particular lab, the dataset is small and the computation is relatively low, so using parallel techniques would yield little aside from experience for the learner, but if you’d like to learn more about ways to implement parallelization when working with pandas, see this link to learn more about Modin, an early state DataFrame library that wraps Pandas and transparently distributes data and computation to accelerate workflows that stand to benefit from distributed computing. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading datasets \n",
    "iris = load_iris() \n",
    " \n",
    "# visualize the data \n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names) \n",
    "df['species'] = np.array([iris.target_names[i] for i in iris.target]) \n",
    "sns.pairplot(df, hue='species') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that setosa’s features are fairly dissimilar from the other two flowers, but versicolor and virginica appear to be a little less distinct.  \n",
    "\n",
    "What do you notice about the data set?  \n",
    "\n",
    "What data structure(s) have been used so far? Why? \n",
    "\n",
    "Next, we will convert our data into a Pandas DataFrame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe \n",
    "iris_data = pd.DataFrame({ \n",
    "    'sepal length':iris.data[:,0], \n",
    "    'sepal width':iris.data[:,1], \n",
    "    'petal length':iris.data[:,2], \n",
    "    'petal width':iris.data[:,3], \n",
    "    'species':iris.target \n",
    "}) \n",
    "iris_data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been converted to a Pandas DataFrame, we will establish the independent (X) and dependent (Y) variables. Pandas DataFrames are mutable, allow for labeled axes, and are relatively simple to perform mathematical operations and transformations on, this makes them ideal for tackling this sort of classification problem. \n",
    "\n",
    "Some of the advantages of using Pandas for data preprocessing (source): \n",
    "\n",
    "- A fast and efficient DataFrame object for data manipulation with integrated indexing; \n",
    "\n",
    "- Tools for reading and writing data between in-memory data structures and different formats: CSV and text files, Microsoft Excel, SQL databases, and the fast HDF5 format; \n",
    "\n",
    "- Intelligent data alignment and integrated handling of missing data: gain automatic label-based alignment in computations and easily manipulate messy data into an orderly form; \n",
    "\n",
    "- Flexible reshaping and pivoting of data sets; \n",
    "\n",
    "- Intelligent label-based slicing, fancy indexing, and subsetting of large data sets; \n",
    "\n",
    "- Columns can be inserted and deleted from data structures for size mutability; \n",
    "\n",
    "- Aggregating or transforming data with a powerful group by engine allowing split-apply-combine operations on data sets; \n",
    "\n",
    "- High performance merging and joining of data sets; \n",
    "\n",
    "- Hierarchical axis indexing provides an intuitive way of working with high-dimensional data in a lower-dimensional data structure; \n",
    "\n",
    "- Time series-functionality: date range generation and frequency conversion, moving window statistics, date shifting and lagging. Even create domain-specific time offsets and join time series without losing data; \n",
    "\n",
    "- Highly optimized for performance, with critical code paths written in Cython or C. \n",
    "\n",
    "- Python with pandas is in use in a wide variety of academic and commercial domains, including Finance, Neuroscience, Economics, Statistics, Advertising, Web Analytics, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting independent (X) and dependent (Y) variables \n",
    "X = iris_data[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features \n",
    "Y = iris_data['species']  # Labels \n",
    " \n",
    "# printing feature data \n",
    "print('\\nFeature data: \\n') \n",
    "print(X[0:5]) \n",
    "print('--------------------------------------------------------') \n",
    "# printing dependent variable values (0 = setosa, 1 = versicolor, 3 = virginica) \n",
    "print('\\nDependent variable values:\\n') \n",
    "print(Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we have a feature matrix comprised of the 4 different measurements and then we have a target vector that represents the dependent variable. 150 rows with either a 0, 1, or 2, depending on the flower. You may have noticed that a vector is simply a one-dimensional array, and tensors are just a generalization of n-dimensional arrays.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll split the data into training and test sets, allocating 30% of the data for testing the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Model Construction**\n",
    "\n",
    " \n",
    "\n",
    "With all of the data preprocessing complete, we will define the random forest classifier, and then fit the training data to the model. Be sure to set a random state value to ensure reproducibility. With the model trained, we can make predictions for our test set to check our model’s accuracy.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining random forest classifier \n",
    "clfr = RandomForestClassifier(random_state = 100) \n",
    "clfr.fit(X_train, y_train) \n",
    " \n",
    "# making prediction \n",
    "Y_pred = clfr.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained and we’ve tested it against our test set, we can take a look at how it performed. We’ll generate an accuracy score that tells us what percentage of our predictions were correct, and we’ll visualize the prediction distribution with a confusion matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking model accuracy \n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, Y_pred)) \n",
    "cm = pd.DataFrame(confusion_matrix(y_test, Y_pred), columns=iris.target_names, index=iris.target_names) \n",
    "sns.heatmap(cm, annot=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what this was really all about: making predictions. We are going to make species predictions with the dimensions of a new, unknown flower.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions on new data \n",
    "species_id = clfr.predict([[5.1, 3.5, 1.4, 0.2]]) \n",
    "iris.target_names[species_id] \n",
    "print(iris.target_names[species_id]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool! \n",
    "\n",
    " \n",
    "\n",
    "Now we’re going to generate a feature importance score to determine which features were the most relevant to the model for making predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining feature importance (e.g. model participation) \n",
    "feature_imp = pd.Series(clfr.feature_importances_,index=iris.feature_names).sort_values(ascending=False) \n",
    "print(feature_imp) \n",
    "# Creating a bar plot to visualize feature participation in model \n",
    "sns.barplot(x=feature_imp, y=feature_imp.index) \n",
    "# use '%matplotlib inline' to plot inline in jupyter notebooks \n",
    "# Add labels to your graph \n",
    "plt.xlabel('Feature Importance Score') \n",
    "plt.ylabel('Features') \n",
    "plt.title(\"Visualizing Important Features\") \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What does the confusion matrix tell us?*\n",
    "\n",
    "*Is our accuracy rate adequate, or should a different method be used? Why or why not?*\n",
    "\n",
    "*What could we do to improve the classification rate?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gini coefficient measures the inequality among values of a frequency distribution. For example, levels of income. A Gini coefficient of zero expresses perfect equality, where all values are the same, while a coefficient of 1 indicates all of the values are concentrated in one area. The Gini Impurity is the default criterion for classification when using the RF algorithm per the sklearn documentation. More information about Gini Impurity can be found here. \n",
    "\n",
    " \n",
    "\n",
    "To wrap up the lab, we will use the dtreeviz library to generate a decision tree diagram that visualizes the nodal splits that define the classification parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20)) \n",
    "_ = tree.plot_tree(clfr.estimators_[0], feature_names=X.columns, filled=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why would it be useful to visualize our classification model this way?*\n",
    "\n",
    " \n",
    "\n",
    "*What data structure is this?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Links Discussing AI/Python Security**\n",
    "\n",
    "https://www.linkedin.com/pulse/ai-infers-unseen-security-vulnerabilities-boris-paskalev/ \n",
    "\n",
    "https://dev.to/leahfb/python-security-top-5-best-practices-2of3 \n",
    "\n",
    "https://www.genieai.co/blog/using-python-type-checking-to-improve-security "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post-lab Questions** \n",
    "\n",
    " \n",
    "\n",
    "1. What was the classification rate? \n",
    "\n",
    " \n",
    "\n",
    "2. What is a Gini coefficient? Why does it approach 0 as the tree moves away from the initial node.  \n",
    "\n",
    " \n",
    "\n",
    "3. What are a few advantages of the RF algorithm that make it suitable for this sort of classification problem? \n",
    "\n",
    " \n",
    "\n",
    "4. What are the data structures used in this lab? \n",
    "\n",
    " \n",
    "\n",
    "5. Did we train our algorithm with a supervised or unsupervised method?  \n",
    "\n",
    " \n",
    "\n",
    "6. What is one way we could we implement parallel computing when using Pandas? "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "nbconvert_exporter": "python"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}